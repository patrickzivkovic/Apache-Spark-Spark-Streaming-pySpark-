{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "from textblob import TextBlob\n",
    "try:\n",
    "  import subprocess\n",
    "except:\n",
    "  %pip install subprocess\n",
    "  import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\opt\\spark') \n",
    "# import necessary packages\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "# we initiate the StreamingContext with 10 second batch interval. #next we initiate our sqlcontext\n",
    "ssc = StreamingContext(sc, 10)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# initiate streaming text from a TCP (socket) source:\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)\n",
    "# lines of tweets with socket_stream window of size 60, or 60 #seconds windows of time\n",
    "lines = socket_stream.window(10)\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = (\"hashtag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "\n",
    "fields2 = (\"text\", \"count\")\n",
    "Sentiment = namedtuple('Sentiment', fields2)\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "( lines.flatMap( lambda text: text.split( \"b'\" ) ) #Splits to a list containing of the single tweets \n",
    "  .map( lambda word: ( word, 1) ) \n",
    "  .map( lambda rec: Sentiment( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 hashtags to a table.\n",
    "  .limit(10).registerTempTable(\"Sentiment\") ) )\n",
    "\n",
    "# here we apply different operations on the tweets and save them to #a temporary sql table\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  # Checks for    hashtag calls  \n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) \n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    " # .reduceByKey( lambda a, b: a + b ) \n",
    " # Stores in a Tweet Object\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) )\n",
    " # Sorts Them in a dataframe\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") )\n",
    " # Registers only top 10 hashtags to a table.\n",
    "  .limit(10).registerTempTable(\"tweets\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search = input(\"Enter searchword: \")\n",
    "#print(\"Search for: \" + search)\n",
    "\n",
    "#overwrite file\n",
    "#f = open(\"var.py\", \"w\")\n",
    "#f.write(\"var1 = '%s'\" % search)\n",
    "#f.close()\n",
    "\n",
    "file = subprocess.Popen(\"job.sh\", shell = True)\n",
    "\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait for data to stream...\n"
     ]
    }
   ],
   "source": [
    "print(\"Wait for data to stream...\")\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib inline\n",
    "count = 0\n",
    "while count < 5:\n",
    "    \n",
    "    time.sleep(5)\n",
    "    # top hashtags:\n",
    "    top_10_tags = sqlContext.sql( 'Select hashtag, count from tweets' )\n",
    "    top_10_df = top_10_tags.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"hashtag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    #sentiment analysis:\n",
    "    top_10_senti = sqlContext.sql( 'Select text from sentiment' )\n",
    "    top_10_s = top_10_senti.toPandas()\n",
    "    print(top_10_s)\n",
    "    count = count + 1\n",
    "    print(count)\n",
    "    # Schleife Ã¼ber alle texte, die in top_10_s gespeichert sind, und davon jeweils die sentiment analysis\n",
    "    n = 9\n",
    "    s = 0\n",
    "    i = 1\n",
    "    while i <= n:\n",
    "        s = s + i\n",
    "        opinion = TextBlob(top_10_s[\"text\"][i])\n",
    "        print(opinion.sentiment)\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
